<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>LLM on Andrea Grandi</title>
        <link>https://www.andreagrandi.it/tags/llm/</link>
        <description>Recent content in LLM on Andrea Grandi</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 09 Mar 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://www.andreagrandi.it/tags/llm/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Using AI tools for coding: good or bad?</title>
        <link>https://www.andreagrandi.it/posts/using-ai-tools-for-coding-good-or-bad/</link>
        <pubDate>Sat, 09 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>https://www.andreagrandi.it/posts/using-ai-tools-for-coding-good-or-bad/</guid>
        <description>&lt;img src="https://www.andreagrandi.it/posts/using-ai-tools-for-coding-good-or-bad/ai-tools-cover.webp" alt="Featured image of post Using AI tools for coding: good or bad?" /&gt;&lt;p&gt;Every now and then, my Mastodon or LinkedIn timelines are full of messages either praising or condemning the use of AI tools for coding.&lt;/p&gt;
&lt;p&gt;Are they &lt;strong&gt;good or bad&lt;/strong&gt; then? As usual I think the truth is in the middle, or to be more precise: &lt;strong&gt;it depends on how you use them&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context
&lt;/h2&gt;&lt;p&gt;In the last couple of years, thanks to the advancements in the field of &lt;strong&gt;Large Language Models&lt;/strong&gt; (also known as &lt;strong&gt;Generative AI&lt;/strong&gt;) a few &amp;ldquo;AI tools&amp;rdquo; have been made available, to help developers with coding tasks.&lt;/p&gt;
&lt;p&gt;Tools like &lt;strong&gt;GitHub Copilot&lt;/strong&gt; or &lt;strong&gt;ChatGPT&lt;/strong&gt; are being used more frequently by developers. They can either open &lt;strong&gt;new possibilities&lt;/strong&gt; (like helping us to write code in a language we are not familiar with) or just &lt;strong&gt;speed up tasks&lt;/strong&gt; we would do in a different (often slower) way.&lt;/p&gt;
&lt;h2 id=&#34;beware-of-hallucinations&#34;&gt;Beware of hallucinations
&lt;/h2&gt;&lt;p&gt;Before we get into good or bad examples, it&amp;rsquo;s important to remember that by definition, LLMs (large language models) can &lt;strong&gt;hallucinate&lt;/strong&gt;, which means that under certain conditions (which can occur unexpectedly) the &lt;strong&gt;model can make certain things up&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Even ChatGPT reminds us about this, infact at the bottom of every page you will find:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ChatGPT can make mistakes. Consider checking important information.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;an-example-of-good-usage&#34;&gt;An example of Good usage
&lt;/h2&gt;&lt;p&gt;Before the existance of ChatGPT, whenever I had to do something I didn&amp;rsquo;t know exactly how to do it, my first &amp;ldquo;go-to&amp;rdquo; tool was Google.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Example: &amp;ldquo;How do I find duplicated SQL rows with the same field?&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I would search the thing I wanted to do, read a couple of blog posts talking about the subject, figure out if something was possible and then refining my search to find some code examples (usually on Stack Overflow) which then I had to adapt to my existing code base, after reading some documentation.&lt;/p&gt;
&lt;p&gt;Asking the &lt;strong&gt;same question to ChatGPT produced a correct&lt;/strong&gt; (at least in my case) &lt;strong&gt;SQL query along with an explanation&lt;/strong&gt; of the syntax being used (I tested this query on a local database, adapting field names to my own and it produced the expected result).&lt;/p&gt;
&lt;p&gt;I still had to adapt the produced code to my needs (renaming fields, inserting this script in a larger context etc&amp;hellip;) but I certainly &lt;strong&gt;saved a lot of time&lt;/strong&gt; compared to the previous workflow.&lt;/p&gt;
&lt;p&gt;AI tools are &lt;strong&gt;like driving assistance systems&lt;/strong&gt;: &lt;strong&gt;they are not supposed to drive for you&lt;/strong&gt;, but they will help you to maintain the correct distance from the front vehicle, stay below the speed limits, warn you of you start overtaking a car when another one is close behind you etc&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;AI tools can be very helpful with coding tasks. Like any tool, we are in charge of using them correctly and ensuring the end result is accurate. Paying attention to avoid introducing false information or non working code is crucial.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Self hosting a Copilot replacement: my personal experience</title>
        <link>https://www.andreagrandi.it/posts/self-hosting-copilot-replacement/</link>
        <pubDate>Sun, 03 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>https://www.andreagrandi.it/posts/self-hosting-copilot-replacement/</guid>
        <description>&lt;img src="https://www.andreagrandi.it/posts/self-hosting-copilot-replacement/self-hosting-copilot-cover.webp" alt="Featured image of post Self hosting a Copilot replacement: my personal experience" /&gt;&lt;p&gt;For my job (Software Developer) I make a daily use of tools like &lt;a class=&#34;link&#34; href=&#34;https://github.com/features/copilot&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GitHub Copilot&lt;/a&gt; and &lt;a class=&#34;link&#34; href=&#34;https://chat.openai.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ChatGPT&lt;/a&gt;. I want to explore alternative solutions which can be hosted autonomusly, without relying on an external service.&lt;/p&gt;
&lt;h2 id=&#34;disclaimer&#34;&gt;Disclaimer
&lt;/h2&gt;&lt;p&gt;Before I continue, let me clarify that these are &lt;strong&gt;tools&lt;/strong&gt; which makes my job faster, they don&amp;rsquo;t do the job for me: a wrong assumption made by so many people is that AI tools will do the work for you.&lt;/p&gt;
&lt;p&gt;This is quite &lt;strong&gt;not true&lt;/strong&gt;. You are still responsible of knowing what to do, understanding the problem, asking the right questions and knowing what to do with the answers.&lt;/p&gt;
&lt;p&gt;If you regularly search for stuff on Google or any other engine, to find examples in blog posts or Stack Overflow, read someone else example and then adapt the code for your needs, there is nothing wrong automating this: &lt;strong&gt;you are still in charge of the final result&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-experiment&#34;&gt;The experiment
&lt;/h2&gt;&lt;p&gt;After I recently experimented with &lt;a class=&#34;link&#34; href=&#34;https://www.andreagrandi.it/posts/ollama-running-llm-locally/&#34; &gt;local LLMs using Ollama&lt;/a&gt;, I wanted to figure out if I could use some of these models to &lt;strong&gt;replace GitHub Copilot&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;My intent is not to save some money (my company pays for these tools) or for privacy reasons (it would be pointless to keep your code &amp;ldquo;secret&amp;rdquo; from GitHub if you use it to host it), but to &lt;strong&gt;find out how good these alternative tools are&lt;/strong&gt; and&amp;hellip; for fun!&lt;/p&gt;
&lt;h2 id=&#34;setup-and-available-resources&#34;&gt;Setup and available resources
&lt;/h2&gt;&lt;p&gt;The laptop I&amp;rsquo;m using is a MacBook Pro with &lt;strong&gt;M2 Pro&lt;/strong&gt; CPU and &lt;strong&gt;32 GB RAM&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This machine must be able to run my basic &lt;strong&gt;requirements&lt;/strong&gt;, which are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slack&lt;/li&gt;
&lt;li&gt;Docker (running my work containers)&lt;/li&gt;
&lt;li&gt;VSCode&lt;/li&gt;
&lt;li&gt;Safari / Chrome&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s see if it can also run a Copilot replacement.&lt;/p&gt;
&lt;h2 id=&#34;tested-models-and-extensions&#34;&gt;Tested models and extensions
&lt;/h2&gt;&lt;p&gt;Basically, to run a Copilot replacement, you need to be able to run an LLM and use one of the available extensions for VSCode (in my case I use VSCode, but similar extensions exist for other IDEs).&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m running the models locally using &lt;a class=&#34;link&#34; href=&#34;https://ollama.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama&lt;/a&gt;. If you need to know how to use it, you can refer to my &lt;a class=&#34;link&#34; href=&#34;https://www.andreagrandi.it/posts/ollama-running-llm-locally/&#34; &gt;previous post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;models&lt;/strong&gt; I tested are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;stable-code:3b-code-q4_0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;codellama:7b-code-q4_K_M&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;codellama:13b-code-q4_K_M&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The number before the &lt;code&gt;b&lt;/code&gt; usually represents the number of parameters (&lt;strong&gt;3 billions&lt;/strong&gt;, &lt;strong&gt;7 billions&lt;/strong&gt;, &lt;strong&gt;13 billions&lt;/strong&gt; etc&amp;hellip;) and you approximately need &lt;strong&gt;4 GB&lt;/strong&gt;, &lt;strong&gt;8 GB&lt;/strong&gt; and &lt;strong&gt;16 GB RAM&lt;/strong&gt; to use them.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://about.meta.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Meta&lt;/a&gt; has released models with &lt;strong&gt;34 billions&lt;/strong&gt; and &lt;strong&gt;70 billions&lt;/strong&gt; parameters, which are even more powerful, and you can find them here: &lt;a class=&#34;link&#34; href=&#34;https://ollama.com/library/codellama&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ollama.com/library/codellama&lt;/a&gt; but you will need &lt;strong&gt;64-128 GB RAM&lt;/strong&gt; to run them properly.&lt;/p&gt;
&lt;p&gt;The extensions I tested are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LLama Coder&lt;/strong&gt; &lt;a class=&#34;link&#34; href=&#34;https://marketplace.visualstudio.com/items?itemName=ex3ndr.llama-coder&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://marketplace.visualstudio.com/items?itemName=ex3ndr.llama-coder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Code GPT&lt;/strong&gt; &lt;a class=&#34;link&#34; href=&#34;https://marketplace.visualstudio.com/items?itemName=DanielSanMedium.dscodegpt&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://marketplace.visualstudio.com/items?itemName=DanielSanMedium.dscodegpt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;twinny&lt;/strong&gt; &lt;a class=&#34;link&#34; href=&#34;https://marketplace.visualstudio.com/items?itemName=rjmacarthy.twinny&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://marketplace.visualstudio.com/items?itemName=rjmacarthy.twinny&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results
&lt;/h2&gt;&lt;p&gt;The results I got have been quite mixed and &lt;strong&gt;mostly depending on the LLM model&lt;/strong&gt; I was testing, rather than on the extension I was using.&lt;/p&gt;
&lt;p&gt;A model like &lt;code&gt;stable-code:3b-code-q4_0&lt;/code&gt; was quite &lt;strong&gt;fast&lt;/strong&gt; to complete the code I was typing, &lt;strong&gt;but very often&lt;/strong&gt; giving me &lt;strong&gt;wrong&lt;/strong&gt; code or pure garbage (ie: the model wasn&amp;rsquo;t even able to correctly structure a simple Python method or to maintain the correct indentation)&lt;/p&gt;
&lt;p&gt;Models like &lt;code&gt;codellama:7b-code-q4_K_M&lt;/code&gt; or &lt;code&gt;codellama:13b-code-q4_K_M&lt;/code&gt; were giving me &lt;strong&gt;better results&lt;/strong&gt; but despite having 32 GB RAM available and a quite fast CPU, they were &lt;strong&gt;taking 3-4 seconds&lt;/strong&gt; to complete what I was typing, making themselves useless (at least for my use case).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;None of them was even remotely close to the speed and accuracy of GitHub Copilot&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;While the idea of having a personal and private instance of a code assistant is interesting (and can also be the only available option in certain environments), &lt;strong&gt;the reality is that achieving the same level of performance as GitHub Copilot is quite challenging&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Despite these challenges, I think with time both available models and extensions &lt;strong&gt;will get better&lt;/strong&gt; and better, improving their quality and maybe reducing the amount of required resources.&lt;/p&gt;
&lt;p&gt;In case I missed some better model or extension, please feel free to &lt;strong&gt;let me know&lt;/strong&gt; in the comments. I will be glad to do more tests and update this posts or write a new one in the future.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In the mean time&lt;/strong&gt;, at least for my personale usage, I think I will &lt;strong&gt;stick with GitHub Copilot&lt;/strong&gt;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Ollama: running Large Language Models locally</title>
        <link>https://www.andreagrandi.it/posts/ollama-running-llm-locally/</link>
        <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>https://www.andreagrandi.it/posts/ollama-running-llm-locally/</guid>
        <description>&lt;img src="https://www.andreagrandi.it/posts/ollama-running-llm-locally/ollama-cover.webp" alt="Featured image of post Ollama: running Large Language Models locally" /&gt;&lt;p&gt;I recently discovered a new tool called &lt;strong&gt;Ollama&lt;/strong&gt;, which allows you to run &lt;strong&gt;Large Language Models&lt;/strong&gt; (LLMs) locally, without the need of a cloud service. Its usage is similar to Docker, but it&amp;rsquo;s specifically designed for LLMs. You can use it as an &lt;strong&gt;interactive shell&lt;/strong&gt;, through its &lt;strong&gt;REST API&lt;/strong&gt; or using it from a &lt;strong&gt;Python&lt;/strong&gt; library.&lt;/p&gt;
&lt;h2 id=&#34;what-is-ollama&#34;&gt;What is Ollama?
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ollama.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama&lt;/a&gt; is a tool to run and manage Large Language Models locally. It&amp;rsquo;s designed to be easy to use and to be used in different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interactive shell&lt;/strong&gt;: you can run Ollama as a shell and interact with it, you will be able to chat with it, ask questions, and simulate a conversation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;REST API&lt;/strong&gt;: you can run Ollama as a service and send requests to it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Python library&lt;/strong&gt;: you can use Ollama from your Python code.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-does-it-work&#34;&gt;How does it work?
&lt;/h2&gt;&lt;p&gt;After you installa Ollama, you can start chatting with it by simply using this command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama run llama2
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;ollama&lt;/code&gt; will pull the &lt;code&gt;llama2&lt;/code&gt; model from the cloud and start the interactive shell. You can then start chatting with it.&lt;/p&gt;
&lt;h3 id=&#34;demo&#34;&gt;Demo
&lt;/h3&gt;&lt;script src=&#34;https://asciinema.org/a/644736.js&#34; id=&#34;asciicast-644736&#34; async&gt;&lt;/script&gt;
&lt;h2 id=&#34;using-it-through-its-rest-api&#34;&gt;Using it through its REST API
&lt;/h2&gt;&lt;p&gt;Ollama comes with an included &lt;strong&gt;REST API&lt;/strong&gt; which you can use to send requests to it. For example to send a question to the &lt;code&gt;llama2:13b-chat&lt;/code&gt; model you can use the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl http://localhost:11434/api/generate -d &lt;span class=&#34;s1&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;model&amp;#34;: &amp;#34;llama2:13b-chat&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;prompt&amp;#34;: &amp;#34;What can you tell me about Pink Floyd?&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;stream&amp;#34;: false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;}&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;the response will be a &lt;strong&gt;JSON&lt;/strong&gt; object with the generated text, like this one:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;:&lt;span class=&#34;s2&#34;&gt;&amp;#34;llama2:13b-chat&amp;#34;&lt;/span&gt;,&lt;span class=&#34;s2&#34;&gt;&amp;#34;created_at&amp;#34;&lt;/span&gt;:&lt;span class=&#34;s2&#34;&gt;&amp;#34;2024-03-01T17:23:39.094249Z&amp;#34;&lt;/span&gt;,&lt;span class=&#34;s2&#34;&gt;&amp;#34;response&amp;#34;&lt;/span&gt;:&lt;span class=&#34;s2&#34;&gt;&amp;#34;\nAh, Pink Floyd! One of the most iconic and influential rock bands of all time. The band was formed in 1965 by students Syd Barrett, Nick Mason, Roger Waters, and Richard Wright at the University of Cambridge. They began as a folk-rock group, but soon developed their own unique sound that blended psychedelic rock, progressive rock, and experimental music.\n\nPink Floyd is known for their immersive live shows, elaborate album artwork, and thought-provoking lyrics. Their music explores themes such as life, death, technology, and the human condition. Some of their most famous albums include \&amp;#34;The Dark Side of the Moon,\&amp;#34; \&amp;#34;Wish You Were Here,\&amp;#34; and \&amp;#34;The Wall.\&amp;#34;\n\nSyd Barrett, the band&amp;#39;s primary songwriter and lead vocalist, left the band in 1968 due to mental health issues. Roger Waters, who took over as the main songwriter and bassist, became the driving force behind the band&amp;#39;s sound and vision. David Gilmour, a guitarist and vocalist, joined the band in 1967 and remained a key member until the band&amp;#39;s breakup in 1996.\n\nPink Floyd released several critically acclaimed and commercially successful albums throughout their career, including \&amp;#34;Atom Heart Mother,\&amp;#34; \&amp;#34;Meddle,\&amp;#34; \&amp;#34;Obscured by Clouds,\&amp;#34; and \&amp;#34;The Final Cut.\&amp;#34; They are considered one of the greatest rock bands of all time, with a legacy that continues to inspire new generations of musicians and fans.\n\nSome of Pink Floyd&amp;#39;s most famous songs include \&amp;#34;Comfortably Numb,\&amp;#34; \&amp;#34;Another Brick in the Wall (Part 2),\&amp;#34; \&amp;#34;Wish You Were Here,\&amp;#34; and \&amp;#34;Shine On You Crazy Diamond.\&amp;#34; Their music has been covered by countless artists, and their influence can be heard in many different genres of music.\n\nPink Floyd&amp;#39;s live shows were known for their elaborate light shows, lasers, and pyrotechnics. They performed at numerous iconic venues, including the London Coliseum, the Royal Albert Hall, and the Earls Court Exhibition Centre. The band also made several memorable appearances at festivals such as the Bath Festival of Blues and Progressive Music and the Knebworth Festival.\n\nIn 1986, Pink Floyd released \&amp;#34;The Wall,\&amp;#34; a rock opera based on Roger Waters&amp;#39; experiences in school and his feelings about isolation and disillusionment. The album was a huge commercial success and spawned several hit singles, including \&amp;#34;Another Brick in the Wall (Part 2)\&amp;#34; and \&amp;#34;Comfortably Numb.\&amp;#34;\n\nPink Floyd continued to experiment with new sounds and techniques throughout their career. They incorporated elements of electronic music, synthesizers, and sound effects into their recordings, creating a distinctive and innovative sound. The band&amp;#39;s final studio album, \&amp;#34;The Division Bell,\&amp;#34; was released in 1994 and marked the end of Pink Floyd&amp;#39;s active career.\n\nOverall, Pink Floyd is a groundbreaking and influential rock band that has left an indelible mark on the music world. Their unique sound, thought-provoking lyrics, and iconic live shows have made them one of the most beloved and enduring bands of all time.&amp;#34;&lt;/span&gt;,&lt;span class=&#34;s2&#34;&gt;&amp;#34;done&amp;#34;&lt;/span&gt;:true,&lt;span class=&#34;s2&#34;&gt;&amp;#34;context&amp;#34;&lt;/span&gt;:&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;518,25580,29962,3532,14816,29903,29958,5299,829,14816,29903,6778,13,13,5618,508,366,2649,592,1048,349,682,383,18966,29973,518,29914,25580,29962,13,13,17565,29892,349,682,383,18966,29991,3118,310,278,1556,9849,293,322,7112,2556,7679,22706,310,599,931,29889,450,3719,471,8429,297,29871,29896,29929,29953,29945,491,8041,13923,2261,13158,29892,13853,28095,29892,14159,399,10412,29892,322,6123,22927,472,278,3014,310,12585,29889,2688,4689,408,263,19589,29899,20821,2318,29892,541,4720,8906,1009,1914,5412,6047,393,1999,2760,11643,287,295,293,7679,29892,6728,573,7679,29892,322,17986,4696,29889,13,13,29925,682,383,18966,338,2998,363,1009,5198,414,573,5735,3697,29892,19430,3769,1616,1287,29892,322,2714,29899,16123,17223,26627,1199,29889,11275,4696,3902,2361,963,267,1316,408,2834,29892,4892,29892,15483,29892,322,278,5199,4195,29889,3834,310,1009,1556,13834,20618,3160,376,1576,15317,19160,310,278,17549,1699,376,29956,728,887,399,406,2266,1699,322,376,1576,14406,1213,13,13,29903,2941,2261,13158,29892,278,3719,29915,29879,7601,4823,13236,322,3275,20982,391,29892,2175,278,3719,297,29871,29896,29929,29953,29947,2861,304,19119,9045,5626,29889,14159,399,10412,29892,1058,3614,975,408,278,1667,4823,13236,322,12760,391,29892,3897,278,19500,4889,5742,278,3719,29915,29879,6047,322,18551,29889,4699,11788,29885,473,29892,263,11210,391,322,20982,391,29892,8772,278,3719,297,29871,29896,29929,29953,29955,322,9488,263,1820,4509,2745,278,3719,29915,29879,2867,786,297,29871,29896,29929,29929,29953,29889,13,13,29925,682,383,18966,5492,3196,3994,1711,1035,13190,322,7825,5584,9150,20618,10106,1009,6413,29892,3704,376,4178,290,17778,21869,1699,376,19302,29881,280,1699,376,6039,1557,2955,491,14293,29879,1699,322,376,1576,9550,315,329,1213,2688,526,5545,697,310,278,14176,7679,22706,310,599,931,29892,411,263,25000,393,18172,304,8681,533,716,1176,800,310,2301,14722,322,24909,29889,13,13,9526,310,349,682,383,18966,29915,29879,1556,13834,12516,3160,376,1523,3921,2197,405,3774,1699,376,2744,1228,1771,860,297,278,14406,313,7439,29871,29906,511,29908,376,29956,728,887,399,406,2266,1699,322,376,2713,457,1551,887,14279,1537,22904,898,1213,11275,4696,756,1063,10664,491,2302,2222,17906,29892,322,1009,9949,508,367,6091,297,1784,1422,2531,690,310,4696,29889,13,13,29925,682,383,18966,29915,29879,5735,3697,892,2998,363,1009,19430,3578,3697,29892,1869,414,29892,322,11451,4859,3049,1199,29889,2688,8560,472,12727,9849,293,6003,1041,29892,3704,278,4517,1530,895,398,29892,278,7021,10537,6573,29892,322,278,5290,3137,9245,1222,6335,654,11319,29889,450,3719,884,1754,3196,26959,519,21712,472,29482,1338,1316,408,278,28256,8518,310,23434,322,20018,573,6125,322,278,476,484,29890,12554,8518,29889,13,13,797,29871,29896,29929,29947,29953,29892,349,682,383,18966,5492,376,1576,14406,1699,263,7679,14495,2729,373,14159,399,10412,29915,27482,297,3762,322,670,21737,1048,11695,362,322,766,453,3958,358,29889,450,3769,471,263,12176,12128,2551,322,29178,287,3196,7124,22102,29892,3704,376,2744,1228,1771,860,297,278,14406,313,7439,29871,29906,5513,322,376,1523,3921,2197,405,3774,1213,13,13,29925,682,383,18966,7572,304,7639,411,716,10083,322,13698,10106,1009,6413,29889,2688,11039,630,3161,310,27758,4696,29892,14710,267,19427,29892,322,6047,9545,964,1009,2407,886,29892,4969,263,8359,573,322,24233,1230,6047,29889,450,3719,29915,29879,2186,8693,3769,29892,376,1576,7946,10914,1699,471,5492,297,29871,29896,29929,29929,29946,322,10902,278,1095,310,349,682,383,18966,29915,29879,6136,6413,29889,13,13,3563,497,29892,349,682,383,18966,338,263,5962,1030,5086,322,7112,2556,7679,3719,393,756,2175,385,1399,295,1821,2791,373,278,4696,3186,29889,11275,5412,6047,29892,2714,29899,16123,17223,26627,1199,29892,322,9849,293,5735,3697,505,1754,963,697,310,278,1556,1339,8238,322,1095,3864,22706,310,599,931,29889&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;,&lt;span class=&#34;s2&#34;&gt;&amp;#34;total_duration&amp;#34;&lt;/span&gt;:46945500291,&lt;span class=&#34;s2&#34;&gt;&amp;#34;load_duration&amp;#34;&lt;/span&gt;:9055150333,&lt;span class=&#34;s2&#34;&gt;&amp;#34;prompt_eval_count&amp;#34;&lt;/span&gt;:31,&lt;span class=&#34;s2&#34;&gt;&amp;#34;prompt_eval_duration&amp;#34;&lt;/span&gt;:241905000,&lt;span class=&#34;s2&#34;&gt;&amp;#34;eval_count&amp;#34;&lt;/span&gt;:737,&lt;span class=&#34;s2&#34;&gt;&amp;#34;eval_duration&amp;#34;&lt;/span&gt;:37647125000&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;using-it-from-a-python-library&#34;&gt;Using it from a Python library
&lt;/h2&gt;&lt;p&gt;Ollama has libraries for &lt;strong&gt;different programming languages&lt;/strong&gt;, including Python. You can use it from your Python code like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ollama&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ollama&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;llama2:13b-chat&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s1&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Can you talk me about Pink Floyd?&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;stream&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;message&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;additional-capabilities&#34;&gt;Additional capabilities
&lt;/h2&gt;&lt;p&gt;With Ollama you can also create a new model based on an existing one. Check the &lt;a class=&#34;link&#34; href=&#34;https://github.com/ollama/ollama/tree/main?tab=readme-ov-file#customize-a-model&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;official documentation&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;In my examples I used the &lt;code&gt;llama2:13b-chat&lt;/code&gt; model, but there are other models available, you can find the full list &lt;a class=&#34;link&#34; href=&#34;https://ollama.com/library&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;I think Ollama is a great tool for people who want to experiment with Large Language Models without the need of a cloud service. It&amp;rsquo;s easy to use and it&amp;rsquo;s available for different programming languages. I&amp;rsquo;m looking forward to see how it will evolve in the future.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
