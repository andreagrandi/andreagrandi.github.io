<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Ollama on Andrea Grandi</title>
        <link>https://www.andreagrandi.it/tags/ollama/</link>
        <description>Recent content in Ollama on Andrea Grandi</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 26 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://www.andreagrandi.it/tags/ollama/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>How to workaround Ollama models pull issues</title>
        <link>https://www.andreagrandi.it/posts/how-to-workaround-ollama-pull-issues/</link>
        <pubDate>Sun, 26 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://www.andreagrandi.it/posts/how-to-workaround-ollama-pull-issues/</guid>
        <description>&lt;p&gt;In the past couple of days I was having this annoying &lt;strong&gt;issue&lt;/strong&gt; while trying to pull a model from &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://ollama.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama&lt;/a&gt;&lt;/strong&gt;: I was running the command &lt;code&gt;ollama pull deepseek-r1:8b&lt;/code&gt;, it was downloading something like 4-5% of the model and then &lt;strong&gt;the connection was reset&lt;/strong&gt;, the client was &amp;ldquo;crashing&amp;rdquo; and &lt;strong&gt;it always restarted from 0%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;It looks like I wasn&amp;rsquo;t alone: &lt;a class=&#34;link&#34; href=&#34;https://github.com/ollama/ollama/issues/8406&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/ollama/ollama/issues/8406&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: I&amp;rsquo;m running Ollama &lt;code&gt;0.5.7&lt;/code&gt;, it&amp;rsquo;s likely they will fix the issue in one of the next releases.&lt;/p&gt;
&lt;h2 id=&#34;solution&#34;&gt;Solution
&lt;/h2&gt;&lt;p&gt;Someone kindly posted a &lt;a class=&#34;link&#34; href=&#34;https://github.com/ollama/ollama/issues/8535#issuecomment-2613241807&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;workaround&lt;/a&gt; which is a bash script able to invoke Ollama client and resume the download where it was left (Ollama client should to that, but it&amp;rsquo;s not doing it correctly when it crashes).&lt;/p&gt;
&lt;p&gt;Save this script in a file like &lt;code&gt;ollama-pull.sh&lt;/code&gt; and make it executable with &lt;code&gt;chmod +x ollama-pull.sh&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#!/bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;die&lt;span class=&#34;o&#34;&gt;(){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nb&#34;&gt;exit&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#DRYRUN=echo&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;command&lt;/span&gt; -v jq&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; die &lt;span class=&#34;s2&#34;&gt;&amp;#34;Need jq&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;command&lt;/span&gt; -v curl&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; die &lt;span class=&#34;s2&#34;&gt;&amp;#34;Need curl&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; -z &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; die &lt;span class=&#34;s2&#34;&gt;&amp;#34;usage: &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$0&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; modelname&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;%%:*&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[[&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; *:* &lt;span class=&#34;o&#34;&gt;]]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;tag&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;##*:&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;tag&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;latest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;OLLAMA_MODELS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;OLLAMA_MODELS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;-/usr/share/ollama/.ollama/models&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;$OLLAMA_MODELS&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; die &lt;span class=&#34;s2&#34;&gt;&amp;#34;Couldn&amp;#39;t cd to OLLAMA_MODELS (&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$OLLAMA_MODELS&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;)&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; ! -d blobs -o ! -d manifests &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; die &lt;span class=&#34;s2&#34;&gt;&amp;#34;Missing blobs or manifests directory&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;manifest_dir&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;manifests/registry.ollama.ai/library/&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$name&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; -e &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$manifest_dir&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$tag&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; die &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$name&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$tag&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; already exists&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; ! -d &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$manifest_dir&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;$DRYRUN&lt;/span&gt; mkdir -p &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$manifest_dir&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; die &lt;span class=&#34;s2&#34;&gt;&amp;#34;Couldn&amp;#39;t mkdir manifest dir (&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$manifest_dir&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;)&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;manifest&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;curl -sL https://registry.ollama.ai/v2/library/&lt;span class=&#34;nv&#34;&gt;$name&lt;/span&gt;/manifests/&lt;span class=&#34;nv&#34;&gt;$tag&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; die &lt;span class=&#34;s2&#34;&gt;&amp;#34;Couldn&amp;#39;t fetch manifest&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;errors&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;jq -cn &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$manifest&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; |.errors&amp;#34;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$errors&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;null&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; die &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$errors&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;jq -rn &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$manifest&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; | .config.digest&amp;#34;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; die &lt;span class=&#34;s2&#34;&gt;&amp;#34;No config digest&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;$DRYRUN&lt;/span&gt; curl -#L -C - -o blobs/&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;/:/-&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt; https://registry.ollama.ai/v2/library/&lt;span class=&#34;nv&#34;&gt;$name&lt;/span&gt;/blobs/&lt;span class=&#34;nv&#34;&gt;$config&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; die &lt;span class=&#34;s2&#34;&gt;&amp;#34;Couldn&amp;#39;t fetch config blob&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; layer in &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;jq -rn &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$manifest&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; | .layers[].digest&amp;#34;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nv&#34;&gt;$DRYRUN&lt;/span&gt; curl -#L -C - -o blobs/&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;layer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;/:/-&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt; https://registry.ollama.ai/v2/library/&lt;span class=&#34;nv&#34;&gt;$name&lt;/span&gt;/blobs/&lt;span class=&#34;nv&#34;&gt;$layer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; die &lt;span class=&#34;s2&#34;&gt;&amp;#34;Couldn&amp;#39;t fetch layer&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; -n &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$DRYRUN&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;echo &amp;#39;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$manifest&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#39; &amp;gt; &amp;#39;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$manifest_dir&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$tag&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#39;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$manifest&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &amp;gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$manifest_dir&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$tag&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; die &lt;span class=&#34;s2&#34;&gt;&amp;#34;Couldn&amp;#39;t write manifest&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;and run the script in this way:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;OLLAMA_MODELS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;~/.ollama/models ./ollama-pull.sh deepseek-r1:8b
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;(replace &lt;code&gt;deepseek-r1:8b&lt;/code&gt; with the &lt;code&gt;name:tag&lt;/code&gt; of the model you want to pull)&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Self hosting a Copilot replacement: my personal experience</title>
        <link>https://www.andreagrandi.it/posts/self-hosting-copilot-replacement/</link>
        <pubDate>Sun, 03 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>https://www.andreagrandi.it/posts/self-hosting-copilot-replacement/</guid>
        <description>&lt;img src="https://www.andreagrandi.it/posts/self-hosting-copilot-replacement/self-hosting-copilot-cover.webp" alt="Featured image of post Self hosting a Copilot replacement: my personal experience" /&gt;&lt;p&gt;For my job (Software Developer) I make a daily use of tools like &lt;a class=&#34;link&#34; href=&#34;https://github.com/features/copilot&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GitHub Copilot&lt;/a&gt; and &lt;a class=&#34;link&#34; href=&#34;https://chat.openai.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ChatGPT&lt;/a&gt;. I want to explore alternative solutions which can be hosted autonomusly, without relying on an external service.&lt;/p&gt;
&lt;h2 id=&#34;disclaimer&#34;&gt;Disclaimer
&lt;/h2&gt;&lt;p&gt;Before I continue, let me clarify that these are &lt;strong&gt;tools&lt;/strong&gt; which makes my job faster, they don&amp;rsquo;t do the job for me: a wrong assumption made by so many people is that AI tools will do the work for you.&lt;/p&gt;
&lt;p&gt;This is quite &lt;strong&gt;not true&lt;/strong&gt;. You are still responsible of knowing what to do, understanding the problem, asking the right questions and knowing what to do with the answers.&lt;/p&gt;
&lt;p&gt;If you regularly search for stuff on Google or any other engine, to find examples in blog posts or Stack Overflow, read someone else example and then adapt the code for your needs, there is nothing wrong automating this: &lt;strong&gt;you are still in charge of the final result&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-experiment&#34;&gt;The experiment
&lt;/h2&gt;&lt;p&gt;After I recently experimented with &lt;a class=&#34;link&#34; href=&#34;https://www.andreagrandi.it/posts/ollama-running-llm-locally/&#34; &gt;local LLMs using Ollama&lt;/a&gt;, I wanted to figure out if I could use some of these models to &lt;strong&gt;replace GitHub Copilot&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;My intent is not to save some money (my company pays for these tools) or for privacy reasons (it would be pointless to keep your code &amp;ldquo;secret&amp;rdquo; from GitHub if you use it to host it), but to &lt;strong&gt;find out how good these alternative tools are&lt;/strong&gt; and&amp;hellip; for fun!&lt;/p&gt;
&lt;h2 id=&#34;setup-and-available-resources&#34;&gt;Setup and available resources
&lt;/h2&gt;&lt;p&gt;The laptop I&amp;rsquo;m using is a MacBook Pro with &lt;strong&gt;M2 Pro&lt;/strong&gt; CPU and &lt;strong&gt;32 GB RAM&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This machine must be able to run my basic &lt;strong&gt;requirements&lt;/strong&gt;, which are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slack&lt;/li&gt;
&lt;li&gt;Docker (running my work containers)&lt;/li&gt;
&lt;li&gt;VSCode&lt;/li&gt;
&lt;li&gt;Safari / Chrome&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s see if it can also run a Copilot replacement.&lt;/p&gt;
&lt;h2 id=&#34;tested-models-and-extensions&#34;&gt;Tested models and extensions
&lt;/h2&gt;&lt;p&gt;Basically, to run a Copilot replacement, you need to be able to run an LLM and use one of the available extensions for VSCode (in my case I use VSCode, but similar extensions exist for other IDEs).&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m running the models locally using &lt;a class=&#34;link&#34; href=&#34;https://ollama.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama&lt;/a&gt;. If you need to know how to use it, you can refer to my &lt;a class=&#34;link&#34; href=&#34;https://www.andreagrandi.it/posts/ollama-running-llm-locally/&#34; &gt;previous post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;models&lt;/strong&gt; I tested are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;stable-code:3b-code-q4_0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;codellama:7b-code-q4_K_M&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;codellama:13b-code-q4_K_M&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The number before the &lt;code&gt;b&lt;/code&gt; usually represents the number of parameters (&lt;strong&gt;3 billions&lt;/strong&gt;, &lt;strong&gt;7 billions&lt;/strong&gt;, &lt;strong&gt;13 billions&lt;/strong&gt; etc&amp;hellip;) and you approximately need &lt;strong&gt;4 GB&lt;/strong&gt;, &lt;strong&gt;8 GB&lt;/strong&gt; and &lt;strong&gt;16 GB RAM&lt;/strong&gt; to use them.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://about.meta.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Meta&lt;/a&gt; has released models with &lt;strong&gt;34 billions&lt;/strong&gt; and &lt;strong&gt;70 billions&lt;/strong&gt; parameters, which are even more powerful, and you can find them here: &lt;a class=&#34;link&#34; href=&#34;https://ollama.com/library/codellama&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ollama.com/library/codellama&lt;/a&gt; but you will need &lt;strong&gt;64-128 GB RAM&lt;/strong&gt; to run them properly.&lt;/p&gt;
&lt;p&gt;The extensions I tested are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LLama Coder&lt;/strong&gt; &lt;a class=&#34;link&#34; href=&#34;https://marketplace.visualstudio.com/items?itemName=ex3ndr.llama-coder&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://marketplace.visualstudio.com/items?itemName=ex3ndr.llama-coder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Code GPT&lt;/strong&gt; &lt;a class=&#34;link&#34; href=&#34;https://marketplace.visualstudio.com/items?itemName=DanielSanMedium.dscodegpt&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://marketplace.visualstudio.com/items?itemName=DanielSanMedium.dscodegpt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;twinny&lt;/strong&gt; &lt;a class=&#34;link&#34; href=&#34;https://marketplace.visualstudio.com/items?itemName=rjmacarthy.twinny&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://marketplace.visualstudio.com/items?itemName=rjmacarthy.twinny&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results
&lt;/h2&gt;&lt;p&gt;The results I got have been quite mixed and &lt;strong&gt;mostly depending on the LLM model&lt;/strong&gt; I was testing, rather than on the extension I was using.&lt;/p&gt;
&lt;p&gt;A model like &lt;code&gt;stable-code:3b-code-q4_0&lt;/code&gt; was quite &lt;strong&gt;fast&lt;/strong&gt; to complete the code I was typing, &lt;strong&gt;but very often&lt;/strong&gt; giving me &lt;strong&gt;wrong&lt;/strong&gt; code or pure garbage (ie: the model wasn&amp;rsquo;t even able to correctly structure a simple Python method or to maintain the correct indentation)&lt;/p&gt;
&lt;p&gt;Models like &lt;code&gt;codellama:7b-code-q4_K_M&lt;/code&gt; or &lt;code&gt;codellama:13b-code-q4_K_M&lt;/code&gt; were giving me &lt;strong&gt;better results&lt;/strong&gt; but despite having 32 GB RAM available and a quite fast CPU, they were &lt;strong&gt;taking 3-4 seconds&lt;/strong&gt; to complete what I was typing, making themselves useless (at least for my use case).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;None of them was even remotely close to the speed and accuracy of GitHub Copilot&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;While the idea of having a personal and private instance of a code assistant is interesting (and can also be the only available option in certain environments), &lt;strong&gt;the reality is that achieving the same level of performance as GitHub Copilot is quite challenging&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Despite these challenges, I think with time both available models and extensions &lt;strong&gt;will get better&lt;/strong&gt; and better, improving their quality and maybe reducing the amount of required resources.&lt;/p&gt;
&lt;p&gt;In case I missed some better model or extension, please feel free to &lt;strong&gt;let me know&lt;/strong&gt; in the comments. I will be glad to do more tests and update this posts or write a new one in the future.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In the mean time&lt;/strong&gt;, at least for my personale usage, I think I will &lt;strong&gt;stick with GitHub Copilot&lt;/strong&gt;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Ollama: running Large Language Models locally</title>
        <link>https://www.andreagrandi.it/posts/ollama-running-llm-locally/</link>
        <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>https://www.andreagrandi.it/posts/ollama-running-llm-locally/</guid>
        <description>&lt;img src="https://www.andreagrandi.it/posts/ollama-running-llm-locally/ollama-cover.webp" alt="Featured image of post Ollama: running Large Language Models locally" /&gt;&lt;p&gt;I recently discovered a new tool called &lt;strong&gt;Ollama&lt;/strong&gt;, which allows you to run &lt;strong&gt;Large Language Models&lt;/strong&gt; (LLMs) locally, without the need of a cloud service. Its usage is similar to Docker, but it&amp;rsquo;s specifically designed for LLMs. You can use it as an &lt;strong&gt;interactive shell&lt;/strong&gt;, through its &lt;strong&gt;REST API&lt;/strong&gt; or using it from a &lt;strong&gt;Python&lt;/strong&gt; library.&lt;/p&gt;
&lt;h2 id=&#34;what-is-ollama&#34;&gt;What is Ollama?
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ollama.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama&lt;/a&gt; is a tool to run and manage Large Language Models locally. It&amp;rsquo;s designed to be easy to use and to be used in different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interactive shell&lt;/strong&gt;: you can run Ollama as a shell and interact with it, you will be able to chat with it, ask questions, and simulate a conversation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;REST API&lt;/strong&gt;: you can run Ollama as a service and send requests to it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Python library&lt;/strong&gt;: you can use Ollama from your Python code.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-does-it-work&#34;&gt;How does it work?
&lt;/h2&gt;&lt;p&gt;After you installa Ollama, you can start chatting with it by simply using this command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama run llama2
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;ollama&lt;/code&gt; will pull the &lt;code&gt;llama2&lt;/code&gt; model from the cloud and start the interactive shell. You can then start chatting with it.&lt;/p&gt;
&lt;h3 id=&#34;demo&#34;&gt;Demo
&lt;/h3&gt;&lt;script src=&#34;https://asciinema.org/a/644736.js&#34; id=&#34;asciicast-644736&#34; async&gt;&lt;/script&gt;
&lt;h2 id=&#34;using-it-through-its-rest-api&#34;&gt;Using it through its REST API
&lt;/h2&gt;&lt;p&gt;Ollama comes with an included &lt;strong&gt;REST API&lt;/strong&gt; which you can use to send requests to it. For example to send a question to the &lt;code&gt;llama2:13b-chat&lt;/code&gt; model you can use the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl http://localhost:11434/api/generate -d &lt;span class=&#34;s1&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;model&amp;#34;: &amp;#34;llama2:13b-chat&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;prompt&amp;#34;: &amp;#34;What can you tell me about Pink Floyd?&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;stream&amp;#34;: false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;}&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;the response will be a &lt;strong&gt;JSON&lt;/strong&gt; object with the generated text, like this one:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;:&lt;span class=&#34;s2&#34;&gt;&amp;#34;llama2:13b-chat&amp;#34;&lt;/span&gt;,&lt;span class=&#34;s2&#34;&gt;&amp;#34;created_at&amp;#34;&lt;/span&gt;:&lt;span class=&#34;s2&#34;&gt;&amp;#34;2024-03-01T17:23:39.094249Z&amp;#34;&lt;/span&gt;,&lt;span class=&#34;s2&#34;&gt;&amp;#34;response&amp;#34;&lt;/span&gt;:&lt;span class=&#34;s2&#34;&gt;&amp;#34;\nAh, Pink Floyd! One of the most iconic and influential rock bands of all time. The band was formed in 1965 by students Syd Barrett, Nick Mason, Roger Waters, and Richard Wright at the University of Cambridge. They began as a folk-rock group, but soon developed their own unique sound that blended psychedelic rock, progressive rock, and experimental music.\n\nPink Floyd is known for their immersive live shows, elaborate album artwork, and thought-provoking lyrics. Their music explores themes such as life, death, technology, and the human condition. Some of their most famous albums include \&amp;#34;The Dark Side of the Moon,\&amp;#34; \&amp;#34;Wish You Were Here,\&amp;#34; and \&amp;#34;The Wall.\&amp;#34;\n\nSyd Barrett, the band&amp;#39;s primary songwriter and lead vocalist, left the band in 1968 due to mental health issues. Roger Waters, who took over as the main songwriter and bassist, became the driving force behind the band&amp;#39;s sound and vision. David Gilmour, a guitarist and vocalist, joined the band in 1967 and remained a key member until the band&amp;#39;s breakup in 1996.\n\nPink Floyd released several critically acclaimed and commercially successful albums throughout their career, including \&amp;#34;Atom Heart Mother,\&amp;#34; \&amp;#34;Meddle,\&amp;#34; \&amp;#34;Obscured by Clouds,\&amp;#34; and \&amp;#34;The Final Cut.\&amp;#34; They are considered one of the greatest rock bands of all time, with a legacy that continues to inspire new generations of musicians and fans.\n\nSome of Pink Floyd&amp;#39;s most famous songs include \&amp;#34;Comfortably Numb,\&amp;#34; \&amp;#34;Another Brick in the Wall (Part 2),\&amp;#34; \&amp;#34;Wish You Were Here,\&amp;#34; and \&amp;#34;Shine On You Crazy Diamond.\&amp;#34; Their music has been covered by countless artists, and their influence can be heard in many different genres of music.\n\nPink Floyd&amp;#39;s live shows were known for their elaborate light shows, lasers, and pyrotechnics. They performed at numerous iconic venues, including the London Coliseum, the Royal Albert Hall, and the Earls Court Exhibition Centre. The band also made several memorable appearances at festivals such as the Bath Festival of Blues and Progressive Music and the Knebworth Festival.\n\nIn 1986, Pink Floyd released \&amp;#34;The Wall,\&amp;#34; a rock opera based on Roger Waters&amp;#39; experiences in school and his feelings about isolation and disillusionment. The album was a huge commercial success and spawned several hit singles, including \&amp;#34;Another Brick in the Wall (Part 2)\&amp;#34; and \&amp;#34;Comfortably Numb.\&amp;#34;\n\nPink Floyd continued to experiment with new sounds and techniques throughout their career. They incorporated elements of electronic music, synthesizers, and sound effects into their recordings, creating a distinctive and innovative sound. The band&amp;#39;s final studio album, \&amp;#34;The Division Bell,\&amp;#34; was released in 1994 and marked the end of Pink Floyd&amp;#39;s active career.\n\nOverall, Pink Floyd is a groundbreaking and influential rock band that has left an indelible mark on the music world. Their unique sound, thought-provoking lyrics, and iconic live shows have made them one of the most beloved and enduring bands of all time.&amp;#34;&lt;/span&gt;,&lt;span class=&#34;s2&#34;&gt;&amp;#34;done&amp;#34;&lt;/span&gt;:true,&lt;span class=&#34;s2&#34;&gt;&amp;#34;context&amp;#34;&lt;/span&gt;:&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;518,25580,29962,3532,14816,29903,29958,5299,829,14816,29903,6778,13,13,5618,508,366,2649,592,1048,349,682,383,18966,29973,518,29914,25580,29962,13,13,17565,29892,349,682,383,18966,29991,3118,310,278,1556,9849,293,322,7112,2556,7679,22706,310,599,931,29889,450,3719,471,8429,297,29871,29896,29929,29953,29945,491,8041,13923,2261,13158,29892,13853,28095,29892,14159,399,10412,29892,322,6123,22927,472,278,3014,310,12585,29889,2688,4689,408,263,19589,29899,20821,2318,29892,541,4720,8906,1009,1914,5412,6047,393,1999,2760,11643,287,295,293,7679,29892,6728,573,7679,29892,322,17986,4696,29889,13,13,29925,682,383,18966,338,2998,363,1009,5198,414,573,5735,3697,29892,19430,3769,1616,1287,29892,322,2714,29899,16123,17223,26627,1199,29889,11275,4696,3902,2361,963,267,1316,408,2834,29892,4892,29892,15483,29892,322,278,5199,4195,29889,3834,310,1009,1556,13834,20618,3160,376,1576,15317,19160,310,278,17549,1699,376,29956,728,887,399,406,2266,1699,322,376,1576,14406,1213,13,13,29903,2941,2261,13158,29892,278,3719,29915,29879,7601,4823,13236,322,3275,20982,391,29892,2175,278,3719,297,29871,29896,29929,29953,29947,2861,304,19119,9045,5626,29889,14159,399,10412,29892,1058,3614,975,408,278,1667,4823,13236,322,12760,391,29892,3897,278,19500,4889,5742,278,3719,29915,29879,6047,322,18551,29889,4699,11788,29885,473,29892,263,11210,391,322,20982,391,29892,8772,278,3719,297,29871,29896,29929,29953,29955,322,9488,263,1820,4509,2745,278,3719,29915,29879,2867,786,297,29871,29896,29929,29929,29953,29889,13,13,29925,682,383,18966,5492,3196,3994,1711,1035,13190,322,7825,5584,9150,20618,10106,1009,6413,29892,3704,376,4178,290,17778,21869,1699,376,19302,29881,280,1699,376,6039,1557,2955,491,14293,29879,1699,322,376,1576,9550,315,329,1213,2688,526,5545,697,310,278,14176,7679,22706,310,599,931,29892,411,263,25000,393,18172,304,8681,533,716,1176,800,310,2301,14722,322,24909,29889,13,13,9526,310,349,682,383,18966,29915,29879,1556,13834,12516,3160,376,1523,3921,2197,405,3774,1699,376,2744,1228,1771,860,297,278,14406,313,7439,29871,29906,511,29908,376,29956,728,887,399,406,2266,1699,322,376,2713,457,1551,887,14279,1537,22904,898,1213,11275,4696,756,1063,10664,491,2302,2222,17906,29892,322,1009,9949,508,367,6091,297,1784,1422,2531,690,310,4696,29889,13,13,29925,682,383,18966,29915,29879,5735,3697,892,2998,363,1009,19430,3578,3697,29892,1869,414,29892,322,11451,4859,3049,1199,29889,2688,8560,472,12727,9849,293,6003,1041,29892,3704,278,4517,1530,895,398,29892,278,7021,10537,6573,29892,322,278,5290,3137,9245,1222,6335,654,11319,29889,450,3719,884,1754,3196,26959,519,21712,472,29482,1338,1316,408,278,28256,8518,310,23434,322,20018,573,6125,322,278,476,484,29890,12554,8518,29889,13,13,797,29871,29896,29929,29947,29953,29892,349,682,383,18966,5492,376,1576,14406,1699,263,7679,14495,2729,373,14159,399,10412,29915,27482,297,3762,322,670,21737,1048,11695,362,322,766,453,3958,358,29889,450,3769,471,263,12176,12128,2551,322,29178,287,3196,7124,22102,29892,3704,376,2744,1228,1771,860,297,278,14406,313,7439,29871,29906,5513,322,376,1523,3921,2197,405,3774,1213,13,13,29925,682,383,18966,7572,304,7639,411,716,10083,322,13698,10106,1009,6413,29889,2688,11039,630,3161,310,27758,4696,29892,14710,267,19427,29892,322,6047,9545,964,1009,2407,886,29892,4969,263,8359,573,322,24233,1230,6047,29889,450,3719,29915,29879,2186,8693,3769,29892,376,1576,7946,10914,1699,471,5492,297,29871,29896,29929,29929,29946,322,10902,278,1095,310,349,682,383,18966,29915,29879,6136,6413,29889,13,13,3563,497,29892,349,682,383,18966,338,263,5962,1030,5086,322,7112,2556,7679,3719,393,756,2175,385,1399,295,1821,2791,373,278,4696,3186,29889,11275,5412,6047,29892,2714,29899,16123,17223,26627,1199,29892,322,9849,293,5735,3697,505,1754,963,697,310,278,1556,1339,8238,322,1095,3864,22706,310,599,931,29889&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;,&lt;span class=&#34;s2&#34;&gt;&amp;#34;total_duration&amp;#34;&lt;/span&gt;:46945500291,&lt;span class=&#34;s2&#34;&gt;&amp;#34;load_duration&amp;#34;&lt;/span&gt;:9055150333,&lt;span class=&#34;s2&#34;&gt;&amp;#34;prompt_eval_count&amp;#34;&lt;/span&gt;:31,&lt;span class=&#34;s2&#34;&gt;&amp;#34;prompt_eval_duration&amp;#34;&lt;/span&gt;:241905000,&lt;span class=&#34;s2&#34;&gt;&amp;#34;eval_count&amp;#34;&lt;/span&gt;:737,&lt;span class=&#34;s2&#34;&gt;&amp;#34;eval_duration&amp;#34;&lt;/span&gt;:37647125000&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;using-it-from-a-python-library&#34;&gt;Using it from a Python library
&lt;/h2&gt;&lt;p&gt;Ollama has libraries for &lt;strong&gt;different programming languages&lt;/strong&gt;, including Python. You can use it from your Python code like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ollama&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ollama&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;llama2:13b-chat&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s1&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Can you talk me about Pink Floyd?&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;stream&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;message&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;additional-capabilities&#34;&gt;Additional capabilities
&lt;/h2&gt;&lt;p&gt;With Ollama you can also create a new model based on an existing one. Check the &lt;a class=&#34;link&#34; href=&#34;https://github.com/ollama/ollama/tree/main?tab=readme-ov-file#customize-a-model&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;official documentation&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;In my examples I used the &lt;code&gt;llama2:13b-chat&lt;/code&gt; model, but there are other models available, you can find the full list &lt;a class=&#34;link&#34; href=&#34;https://ollama.com/library&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;I think Ollama is a great tool for people who want to experiment with Large Language Models without the need of a cloud service. It&amp;rsquo;s easy to use and it&amp;rsquo;s available for different programming languages. I&amp;rsquo;m looking forward to see how it will evolve in the future.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
