<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Bing on Andrea Grandi</title>
        <link>https://www.andreagrandi.it/tags/bing/</link>
        <description>Recent content in Bing on Andrea Grandi</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 22 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://www.andreagrandi.it/tags/bing/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>We need an evolved robots.txt and regulations to enforce it</title>
        <link>https://www.andreagrandi.it/posts/we-need-evolved-robotstxt-and-regulations/</link>
        <pubDate>Sat, 22 Jun 2024 00:00:00 +0000</pubDate>
        
        <guid>https://www.andreagrandi.it/posts/we-need-evolved-robotstxt-and-regulations/</guid>
        <description>&lt;img src="https://www.andreagrandi.it/posts/we-need-evolved-robotstxt-and-regulations/we-need-new-robots.png" alt="Featured image of post We need an evolved robots.txt and regulations to enforce it" /&gt;&lt;p&gt;The &lt;code&gt;robots.txt&lt;/code&gt; file is a simple text file that tells web robots (like search engine crawlers) which pages on your site to crawl and which not to crawl. It&amp;rsquo;s a &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Robots.txt&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;standard&lt;/a&gt; that has been around for a long time and it&amp;rsquo;s still used today.&lt;/p&gt;
&lt;p&gt;Some examples of rules you can put in a &lt;code&gt;robots.txt&lt;/code&gt; file are:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-txt&#34; data-lang=&#34;txt&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;User-agent: *
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Disallow: /private/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;This rule tells all web robots to not crawl the &lt;code&gt;/private/&lt;/code&gt; directory.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-txt&#34; data-lang=&#34;txt&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;User-agent: Googlebot
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Disallow: /users/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;This rule tells Googlebot to not crawl the &lt;code&gt;/users/&lt;/code&gt; directory.&lt;/p&gt;
&lt;h2 id=&#34;then-ai-came&#34;&gt;Then AI came
&lt;/h2&gt;&lt;p&gt;In the age of AI, the existing &lt;code&gt;robots.txt&lt;/code&gt; specification is not enough to express the rules for web crawlers. We can only tell agent if they can or cannot crawl a certain path, but we cannot express more complex rules.&lt;/p&gt;
&lt;p&gt;In my opinion we should be able to express more detailed rules, like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Indexing:&lt;/strong&gt; should a web crawler be able to index the content?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Caching:&lt;/strong&gt; should a web crawler be able to cache the content?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LLM Training:&lt;/strong&gt; should a web crawler be able to use the content to train a language model?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Summarising:&lt;/strong&gt; should a web crawler be able to summarise the content?&lt;/li&gt;
&lt;li&gt;etc&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the above things were not possible in the past and it should be up to the website owner to decide if they want their content to be used in such ways.&lt;/p&gt;
&lt;h2 id=&#34;enforcing-the-rules&#34;&gt;Enforcing the rules
&lt;/h2&gt;&lt;p&gt;In addition to more detailed rules, &lt;strong&gt;we need new regulations to enforce them&lt;/strong&gt;. It looks like the &lt;code&gt;robots.txt&lt;/code&gt; file is not enough to stop certain companies from doing what they want.&lt;/p&gt;
&lt;p&gt;As someone &lt;a class=&#34;link&#34; href=&#34;https://rknight.me/blog/perplexity-ai-is-lying-about-its-user-agent/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;recently found out&lt;/a&gt;, &lt;strong&gt;Perplexity AI is using a fake user agent to crawl websites&lt;/strong&gt;, pretending to be a regular user. This is a clear violation of the rules specified in &lt;code&gt;robots.txt&lt;/code&gt; file. This claim has recently been &lt;a class=&#34;link&#34; href=&#34;https://www.wired.com/story/perplexity-is-a-bullshit-machine/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;confirmed by Wired&lt;/a&gt; and by &lt;a class=&#34;link&#34; href=&#34;https://www.macstories.net/stories/wired-confirms-perplexity-is-bypassing-efforts-by-websites-to-block-its-web-crawler/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MacStories&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;As we have seen, having good rules is not enough if they are not enforced. In particular we need regulators to take care of complaints from content owners and &lt;strong&gt;fine companies that do not respect the rules&lt;/strong&gt; (like Perplexity AI), because small content creators cannot afford to take legal actions against big companies.&lt;/p&gt;
&lt;p&gt;As with every single thing, it&amp;rsquo;s never &amp;ldquo;the tool&amp;rdquo;, but rather &amp;ldquo;how you use it&amp;rdquo;. AI itself can bring innovation in certain fields, but this can&amp;rsquo;t be done at the expense of other people&amp;rsquo;s work and rights.&lt;/p&gt;
&lt;h3 id=&#34;disclaimer&#34;&gt;Disclaimer
&lt;/h3&gt;&lt;p&gt;Yes, of course the cover image has been generated with AI. It&amp;rsquo;s far from perfect, but it&amp;rsquo;s still better than my drawing skills. The content of this article instead is 100% human generated.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
