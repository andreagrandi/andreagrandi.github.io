<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Tutorial on Andrea Grandi</title>
        <link>https://www.andreagrandi.it/categories/tutorial/</link>
        <description>Recent content in Tutorial on Andrea Grandi</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 03 Mar 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://www.andreagrandi.it/categories/tutorial/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Self hosting a Copilot replacement: my personal experience</title>
        <link>https://www.andreagrandi.it/posts/self-hosting-copilot-replacement/</link>
        <pubDate>Sun, 03 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>https://www.andreagrandi.it/posts/self-hosting-copilot-replacement/</guid>
        <description>&lt;img src="https://www.andreagrandi.it/posts/self-hosting-copilot-replacement/self-hosting-copilot-cover.webp" alt="Featured image of post Self hosting a Copilot replacement: my personal experience" /&gt;&lt;p&gt;For my job (Software Developer) I make a daily use of tools like &lt;a class=&#34;link&#34; href=&#34;https://github.com/features/copilot&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GitHub Copilot&lt;/a&gt; and &lt;a class=&#34;link&#34; href=&#34;https://chat.openai.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ChatGPT&lt;/a&gt;. I want to explore alternative solutions which can be hosted autonomusly, without relying on an external service.&lt;/p&gt;
&lt;h2 id=&#34;disclaimer&#34;&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;Before I continue, let me clarify that these are &lt;strong&gt;tools&lt;/strong&gt; which makes my job faster, they don&amp;rsquo;t do the job for me: a wrong assumption made by so many people is that AI tools will do the work for you.&lt;/p&gt;
&lt;p&gt;This is quite &lt;strong&gt;not true&lt;/strong&gt;. You are still responsible of knowing what to do, understanding the problem, asking the right questions and knowing what to do with the answers.&lt;/p&gt;
&lt;p&gt;If you regularly search for stuff on Google or any other engine, to find examples in blog posts or Stack Overflow, read someone else example and then adapt the code for your needs, there is nothing wrong automating this: &lt;strong&gt;you are still in charge of the final result&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-experiment&#34;&gt;The experiment&lt;/h2&gt;
&lt;p&gt;After I recently experimented with &lt;a class=&#34;link&#34; href=&#34;https://www.andreagrandi.it/posts/ollama-running-llm-locally/&#34; &gt;local LLMs using Ollama&lt;/a&gt;, I wanted to figure out if I could use some of these models to &lt;strong&gt;replace GitHub Copilot&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;My intent is not to save some money (my company pays for these tools) or for privacy reasons (it would be pointless to keep your code &amp;ldquo;secret&amp;rdquo; from GitHub if you use it to host it), but to &lt;strong&gt;find out how good these alternative tools are&lt;/strong&gt; and&amp;hellip; for fun!&lt;/p&gt;
&lt;h2 id=&#34;setup-and-available-resources&#34;&gt;Setup and available resources&lt;/h2&gt;
&lt;p&gt;The laptop I&amp;rsquo;m using is a MacBook Pro with &lt;strong&gt;M2 Pro&lt;/strong&gt; CPU and &lt;strong&gt;32 GB RAM&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This machine must be able to run my basic &lt;strong&gt;requirements&lt;/strong&gt;, which are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slack&lt;/li&gt;
&lt;li&gt;Docker (running my work containers)&lt;/li&gt;
&lt;li&gt;VSCode&lt;/li&gt;
&lt;li&gt;Safari / Chrome&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s see if it can also run a Copilot replacement.&lt;/p&gt;
&lt;h2 id=&#34;tested-models-and-extensions&#34;&gt;Tested models and extensions&lt;/h2&gt;
&lt;p&gt;Basically, to run a Copilot replacement, you need to be able to run an LLM and use one of the available extensions for VSCode (in my case I use VSCode, but similar extensions exist for other IDEs).&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m running the models locally using &lt;a class=&#34;link&#34; href=&#34;https://ollama.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama&lt;/a&gt;. If you need to know how to use it, you can refer to my &lt;a class=&#34;link&#34; href=&#34;https://www.andreagrandi.it/posts/ollama-running-llm-locally/&#34; &gt;previous post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;models&lt;/strong&gt; I tested are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;stable-code:3b-code-q4_0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;codellama:7b-code-q4_K_M&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;codellama:13b-code-q4_K_M&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The number before the &lt;code&gt;b&lt;/code&gt; usually represents the number of parameters (&lt;strong&gt;3 billions&lt;/strong&gt;, &lt;strong&gt;7 billions&lt;/strong&gt;, &lt;strong&gt;13 billions&lt;/strong&gt; etc&amp;hellip;) and you approximately need &lt;strong&gt;4 GB&lt;/strong&gt;, &lt;strong&gt;8 GB&lt;/strong&gt; and &lt;strong&gt;16 GB RAM&lt;/strong&gt; to use them.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://about.meta.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Meta&lt;/a&gt; has released models with &lt;strong&gt;34 billions&lt;/strong&gt; and &lt;strong&gt;70 billions&lt;/strong&gt; parameters, which are even more powerful, and you can find them here: &lt;a class=&#34;link&#34; href=&#34;https://ollama.com/library/codellama&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ollama.com/library/codellama&lt;/a&gt; but you will need &lt;strong&gt;64-128 GB RAM&lt;/strong&gt; to run them properly.&lt;/p&gt;
&lt;p&gt;The extensions I tested are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LLama Coder&lt;/strong&gt; &lt;a class=&#34;link&#34; href=&#34;https://marketplace.visualstudio.com/items?itemName=ex3ndr.llama-coder&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://marketplace.visualstudio.com/items?itemName=ex3ndr.llama-coder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Code GPT&lt;/strong&gt; &lt;a class=&#34;link&#34; href=&#34;https://marketplace.visualstudio.com/items?itemName=DanielSanMedium.dscodegpt&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://marketplace.visualstudio.com/items?itemName=DanielSanMedium.dscodegpt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;twinny&lt;/strong&gt; &lt;a class=&#34;link&#34; href=&#34;https://marketplace.visualstudio.com/items?itemName=rjmacarthy.twinny&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://marketplace.visualstudio.com/items?itemName=rjmacarthy.twinny&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The results I got have been quite mixed and &lt;strong&gt;mostly depending on the LLM model&lt;/strong&gt; I was testing, rather than on the extension I was using.&lt;/p&gt;
&lt;p&gt;A model like &lt;code&gt;stable-code:3b-code-q4_0&lt;/code&gt; was quite &lt;strong&gt;fast&lt;/strong&gt; to complete the code I was typing, &lt;strong&gt;but very often&lt;/strong&gt; giving me &lt;strong&gt;wrong&lt;/strong&gt; code or pure garbage (ie: the model wasn&amp;rsquo;t even able to correctly structure a simple Python method or to maintain the correct indentation)&lt;/p&gt;
&lt;p&gt;Models like &lt;code&gt;codellama:7b-code-q4_K_M&lt;/code&gt; or &lt;code&gt;codellama:13b-code-q4_K_M&lt;/code&gt; were giving me &lt;strong&gt;better results&lt;/strong&gt; but despite having 32 GB RAM available and a quite fast CPU, they were &lt;strong&gt;taking 3-4 seconds&lt;/strong&gt; to complete what I was typing, making themselves useless (at least for my use case).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;None of them was even remotely close to the speed and accuracy of GitHub Copilot&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;While the idea of having a personal and private instance of a code assistant is interesting (and can also be the only available option in certain environments), &lt;strong&gt;the reality is that achieving the same level of performance as GitHub Copilot is quite challenging&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Despite these challenges, I think with time both available models and extensions &lt;strong&gt;will get better&lt;/strong&gt; and better, improving their quality and maybe reducing the amount of required resources.&lt;/p&gt;
&lt;p&gt;In case I missed some better model or extension, please feel free to &lt;strong&gt;let me know&lt;/strong&gt; in the comments. I will be glad to do more tests and update this posts or write a new one in the future.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In the mean time&lt;/strong&gt;, at least for my personale usage, I think I will &lt;strong&gt;stick with GitHub Copilot&lt;/strong&gt;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Ollama: running Large Language Models locally</title>
        <link>https://www.andreagrandi.it/posts/ollama-running-llm-locally/</link>
        <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>https://www.andreagrandi.it/posts/ollama-running-llm-locally/</guid>
        <description>&lt;img src="https://www.andreagrandi.it/posts/ollama-running-llm-locally/ollama-cover.webp" alt="Featured image of post Ollama: running Large Language Models locally" /&gt;&lt;p&gt;I recently discovered a new tool called &lt;strong&gt;Ollama&lt;/strong&gt;, which allows you to run &lt;strong&gt;Large Language Models&lt;/strong&gt; (LLMs) locally, without the need of a cloud service. Its usage is similar to Docker, but it&amp;rsquo;s specifically designed for LLMs. You can use it as an &lt;strong&gt;interactive shell&lt;/strong&gt;, through its &lt;strong&gt;REST API&lt;/strong&gt; or using it from a &lt;strong&gt;Python&lt;/strong&gt; library.&lt;/p&gt;
&lt;h2 id=&#34;what-is-ollama&#34;&gt;What is Ollama?&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ollama.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama&lt;/a&gt; is a tool to run and manage Large Language Models locally. It&amp;rsquo;s designed to be easy to use and to be used in different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interactive shell&lt;/strong&gt;: you can run Ollama as a shell and interact with it, you will be able to chat with it, ask questions, and simulate a conversation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;REST API&lt;/strong&gt;: you can run Ollama as a service and send requests to it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Python library&lt;/strong&gt;: you can use Ollama from your Python code.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-does-it-work&#34;&gt;How does it work?&lt;/h2&gt;
&lt;p&gt;After you installa Ollama, you can start chatting with it by simply using this command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama run llama2
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;ollama&lt;/code&gt; will pull the &lt;code&gt;llama2&lt;/code&gt; model from the cloud and start the interactive shell. You can then start chatting with it.&lt;/p&gt;
&lt;h3 id=&#34;demo&#34;&gt;Demo&lt;/h3&gt;
&lt;script src=&#34;https://asciinema.org/a/644736.js&#34; id=&#34;asciicast-644736&#34; async&gt;&lt;/script&gt;
&lt;h2 id=&#34;using-it-through-its-rest-api&#34;&gt;Using it through its REST API&lt;/h2&gt;
&lt;p&gt;Ollama comes with an included &lt;strong&gt;REST API&lt;/strong&gt; which you can use to send requests to it. For example to send a question to the &lt;code&gt;llama2:13b-chat&lt;/code&gt; model you can use the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl http://localhost:11434/api/generate -d &lt;span class=&#34;s1&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;model&amp;#34;: &amp;#34;llama2:13b-chat&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;prompt&amp;#34;: &amp;#34;What can you tell me about Pink Floyd?&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  &amp;#34;stream&amp;#34;: false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;}&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;the response will be a &lt;strong&gt;JSON&lt;/strong&gt; object with the generated text, like this one:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;:&lt;span class=&#34;s2&#34;&gt;&amp;#34;llama2:13b-chat&amp;#34;&lt;/span&gt;,&lt;span class=&#34;s2&#34;&gt;&amp;#34;created_at&amp;#34;&lt;/span&gt;:&lt;span class=&#34;s2&#34;&gt;&amp;#34;2024-03-01T17:23:39.094249Z&amp;#34;&lt;/span&gt;,&lt;span class=&#34;s2&#34;&gt;&amp;#34;response&amp;#34;&lt;/span&gt;:&lt;span class=&#34;s2&#34;&gt;&amp;#34;\nAh, Pink Floyd! One of the most iconic and influential rock bands of all time. The band was formed in 1965 by students Syd Barrett, Nick Mason, Roger Waters, and Richard Wright at the University of Cambridge. They began as a folk-rock group, but soon developed their own unique sound that blended psychedelic rock, progressive rock, and experimental music.\n\nPink Floyd is known for their immersive live shows, elaborate album artwork, and thought-provoking lyrics. Their music explores themes such as life, death, technology, and the human condition. Some of their most famous albums include \&amp;#34;The Dark Side of the Moon,\&amp;#34; \&amp;#34;Wish You Were Here,\&amp;#34; and \&amp;#34;The Wall.\&amp;#34;\n\nSyd Barrett, the band&amp;#39;s primary songwriter and lead vocalist, left the band in 1968 due to mental health issues. Roger Waters, who took over as the main songwriter and bassist, became the driving force behind the band&amp;#39;s sound and vision. David Gilmour, a guitarist and vocalist, joined the band in 1967 and remained a key member until the band&amp;#39;s breakup in 1996.\n\nPink Floyd released several critically acclaimed and commercially successful albums throughout their career, including \&amp;#34;Atom Heart Mother,\&amp;#34; \&amp;#34;Meddle,\&amp;#34; \&amp;#34;Obscured by Clouds,\&amp;#34; and \&amp;#34;The Final Cut.\&amp;#34; They are considered one of the greatest rock bands of all time, with a legacy that continues to inspire new generations of musicians and fans.\n\nSome of Pink Floyd&amp;#39;s most famous songs include \&amp;#34;Comfortably Numb,\&amp;#34; \&amp;#34;Another Brick in the Wall (Part 2),\&amp;#34; \&amp;#34;Wish You Were Here,\&amp;#34; and \&amp;#34;Shine On You Crazy Diamond.\&amp;#34; Their music has been covered by countless artists, and their influence can be heard in many different genres of music.\n\nPink Floyd&amp;#39;s live shows were known for their elaborate light shows, lasers, and pyrotechnics. They performed at numerous iconic venues, including the London Coliseum, the Royal Albert Hall, and the Earls Court Exhibition Centre. The band also made several memorable appearances at festivals such as the Bath Festival of Blues and Progressive Music and the Knebworth Festival.\n\nIn 1986, Pink Floyd released \&amp;#34;The Wall,\&amp;#34; a rock opera based on Roger Waters&amp;#39; experiences in school and his feelings about isolation and disillusionment. The album was a huge commercial success and spawned several hit singles, including \&amp;#34;Another Brick in the Wall (Part 2)\&amp;#34; and \&amp;#34;Comfortably Numb.\&amp;#34;\n\nPink Floyd continued to experiment with new sounds and techniques throughout their career. They incorporated elements of electronic music, synthesizers, and sound effects into their recordings, creating a distinctive and innovative sound. The band&amp;#39;s final studio album, \&amp;#34;The Division Bell,\&amp;#34; was released in 1994 and marked the end of Pink Floyd&amp;#39;s active career.\n\nOverall, Pink Floyd is a groundbreaking and influential rock band that has left an indelible mark on the music world. Their unique sound, thought-provoking lyrics, and iconic live shows have made them one of the most beloved and enduring bands of all time.&amp;#34;&lt;/span&gt;,&lt;span class=&#34;s2&#34;&gt;&amp;#34;done&amp;#34;&lt;/span&gt;:true,&lt;span class=&#34;s2&#34;&gt;&amp;#34;context&amp;#34;&lt;/span&gt;:&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;518,25580,29962,3532,14816,29903,29958,5299,829,14816,29903,6778,13,13,5618,508,366,2649,592,1048,349,682,383,18966,29973,518,29914,25580,29962,13,13,17565,29892,349,682,383,18966,29991,3118,310,278,1556,9849,293,322,7112,2556,7679,22706,310,599,931,29889,450,3719,471,8429,297,29871,29896,29929,29953,29945,491,8041,13923,2261,13158,29892,13853,28095,29892,14159,399,10412,29892,322,6123,22927,472,278,3014,310,12585,29889,2688,4689,408,263,19589,29899,20821,2318,29892,541,4720,8906,1009,1914,5412,6047,393,1999,2760,11643,287,295,293,7679,29892,6728,573,7679,29892,322,17986,4696,29889,13,13,29925,682,383,18966,338,2998,363,1009,5198,414,573,5735,3697,29892,19430,3769,1616,1287,29892,322,2714,29899,16123,17223,26627,1199,29889,11275,4696,3902,2361,963,267,1316,408,2834,29892,4892,29892,15483,29892,322,278,5199,4195,29889,3834,310,1009,1556,13834,20618,3160,376,1576,15317,19160,310,278,17549,1699,376,29956,728,887,399,406,2266,1699,322,376,1576,14406,1213,13,13,29903,2941,2261,13158,29892,278,3719,29915,29879,7601,4823,13236,322,3275,20982,391,29892,2175,278,3719,297,29871,29896,29929,29953,29947,2861,304,19119,9045,5626,29889,14159,399,10412,29892,1058,3614,975,408,278,1667,4823,13236,322,12760,391,29892,3897,278,19500,4889,5742,278,3719,29915,29879,6047,322,18551,29889,4699,11788,29885,473,29892,263,11210,391,322,20982,391,29892,8772,278,3719,297,29871,29896,29929,29953,29955,322,9488,263,1820,4509,2745,278,3719,29915,29879,2867,786,297,29871,29896,29929,29929,29953,29889,13,13,29925,682,383,18966,5492,3196,3994,1711,1035,13190,322,7825,5584,9150,20618,10106,1009,6413,29892,3704,376,4178,290,17778,21869,1699,376,19302,29881,280,1699,376,6039,1557,2955,491,14293,29879,1699,322,376,1576,9550,315,329,1213,2688,526,5545,697,310,278,14176,7679,22706,310,599,931,29892,411,263,25000,393,18172,304,8681,533,716,1176,800,310,2301,14722,322,24909,29889,13,13,9526,310,349,682,383,18966,29915,29879,1556,13834,12516,3160,376,1523,3921,2197,405,3774,1699,376,2744,1228,1771,860,297,278,14406,313,7439,29871,29906,511,29908,376,29956,728,887,399,406,2266,1699,322,376,2713,457,1551,887,14279,1537,22904,898,1213,11275,4696,756,1063,10664,491,2302,2222,17906,29892,322,1009,9949,508,367,6091,297,1784,1422,2531,690,310,4696,29889,13,13,29925,682,383,18966,29915,29879,5735,3697,892,2998,363,1009,19430,3578,3697,29892,1869,414,29892,322,11451,4859,3049,1199,29889,2688,8560,472,12727,9849,293,6003,1041,29892,3704,278,4517,1530,895,398,29892,278,7021,10537,6573,29892,322,278,5290,3137,9245,1222,6335,654,11319,29889,450,3719,884,1754,3196,26959,519,21712,472,29482,1338,1316,408,278,28256,8518,310,23434,322,20018,573,6125,322,278,476,484,29890,12554,8518,29889,13,13,797,29871,29896,29929,29947,29953,29892,349,682,383,18966,5492,376,1576,14406,1699,263,7679,14495,2729,373,14159,399,10412,29915,27482,297,3762,322,670,21737,1048,11695,362,322,766,453,3958,358,29889,450,3769,471,263,12176,12128,2551,322,29178,287,3196,7124,22102,29892,3704,376,2744,1228,1771,860,297,278,14406,313,7439,29871,29906,5513,322,376,1523,3921,2197,405,3774,1213,13,13,29925,682,383,18966,7572,304,7639,411,716,10083,322,13698,10106,1009,6413,29889,2688,11039,630,3161,310,27758,4696,29892,14710,267,19427,29892,322,6047,9545,964,1009,2407,886,29892,4969,263,8359,573,322,24233,1230,6047,29889,450,3719,29915,29879,2186,8693,3769,29892,376,1576,7946,10914,1699,471,5492,297,29871,29896,29929,29929,29946,322,10902,278,1095,310,349,682,383,18966,29915,29879,6136,6413,29889,13,13,3563,497,29892,349,682,383,18966,338,263,5962,1030,5086,322,7112,2556,7679,3719,393,756,2175,385,1399,295,1821,2791,373,278,4696,3186,29889,11275,5412,6047,29892,2714,29899,16123,17223,26627,1199,29892,322,9849,293,5735,3697,505,1754,963,697,310,278,1556,1339,8238,322,1095,3864,22706,310,599,931,29889&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;,&lt;span class=&#34;s2&#34;&gt;&amp;#34;total_duration&amp;#34;&lt;/span&gt;:46945500291,&lt;span class=&#34;s2&#34;&gt;&amp;#34;load_duration&amp;#34;&lt;/span&gt;:9055150333,&lt;span class=&#34;s2&#34;&gt;&amp;#34;prompt_eval_count&amp;#34;&lt;/span&gt;:31,&lt;span class=&#34;s2&#34;&gt;&amp;#34;prompt_eval_duration&amp;#34;&lt;/span&gt;:241905000,&lt;span class=&#34;s2&#34;&gt;&amp;#34;eval_count&amp;#34;&lt;/span&gt;:737,&lt;span class=&#34;s2&#34;&gt;&amp;#34;eval_duration&amp;#34;&lt;/span&gt;:37647125000&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;using-it-from-a-python-library&#34;&gt;Using it from a Python library&lt;/h2&gt;
&lt;p&gt;Ollama has libraries for &lt;strong&gt;different programming languages&lt;/strong&gt;, including Python. You can use it from your Python code like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ollama&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ollama&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;llama2:13b-chat&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s1&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Can you talk me about Pink Floyd?&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;stream&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;message&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;additional-capabilities&#34;&gt;Additional capabilities&lt;/h2&gt;
&lt;p&gt;With Ollama you can also create a new model based on an existing one. Check the &lt;a class=&#34;link&#34; href=&#34;https://github.com/ollama/ollama/tree/main?tab=readme-ov-file#customize-a-model&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;official documentation&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;In my examples I used the &lt;code&gt;llama2:13b-chat&lt;/code&gt; model, but there are other models available, you can find the full list &lt;a class=&#34;link&#34; href=&#34;https://ollama.com/library&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I think Ollama is a great tool for people who want to experiment with Large Language Models without the need of a cloud service. It&amp;rsquo;s easy to use and it&amp;rsquo;s available for different programming languages. I&amp;rsquo;m looking forward to see how it will evolve in the future.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>My Backup Strategy with Borg, Vorta and BorgBase</title>
        <link>https://www.andreagrandi.it/posts/my-backup-strategy/</link>
        <pubDate>Wed, 28 Feb 2024 00:00:00 +0000</pubDate>
        
        <guid>https://www.andreagrandi.it/posts/my-backup-strategy/</guid>
        <description>&lt;img src="https://www.andreagrandi.it/posts/my-backup-strategy/backup-strategy-cover.webp" alt="Featured image of post My Backup Strategy with Borg, Vorta and BorgBase" /&gt;&lt;p&gt;Recently I reviewed my backup strategy and started using a new open source software and a cloud service to keep my data safe.&lt;/p&gt;
&lt;p&gt;Before this my only backup was keeping my data in the cloud (specifically I use pCloud), but I started thinking: what if they shut down? What if I loose access to my account?&lt;/p&gt;
&lt;p&gt;This convinced me to have an additional copy of my data.&lt;/p&gt;
&lt;p&gt;Actually two additional copies: a local one, on an &lt;strong&gt;USB disk&lt;/strong&gt;, which is of course not reliable (disks can and will break!), but it&amp;rsquo;s fast if I need to recover something and a remote one, on a &lt;strong&gt;dedicated cloud service&lt;/strong&gt;, which is of course slower but quite reliable (you pay for your data to be kept safe).&lt;/p&gt;
&lt;h2 id=&#34;my-personal-requirements&#34;&gt;My personal requirements&lt;/h2&gt;
&lt;p&gt;Requirements can be different from person to person, but in this case I think they are quite common sense.&lt;/p&gt;
&lt;p&gt;The first thing I want from a backup solution is that it is &lt;strong&gt;encrypted&lt;/strong&gt; and that I will be the only one having the encryption keys.&lt;/p&gt;
&lt;p&gt;The second requirement is that the backup is &lt;strong&gt;incremental&lt;/strong&gt;. This means that if I do a new backup every day, I expect the new backup to only contain the data which changed from the previous one.&lt;/p&gt;
&lt;p&gt;The third requirement is that the backup can be &lt;strong&gt;automated&lt;/strong&gt;. I donâ€™t want to have to remember to backup my data, I want it to happen automatically.&lt;/p&gt;
&lt;p&gt;In addition to the above, I have a strong preference for solutions which are &lt;strong&gt;open source&lt;/strong&gt; and possibly &lt;strong&gt;free&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;borgbackup&#34;&gt;BorgBackup&lt;/h2&gt;
&lt;p&gt;The solution which matches all the above requirements is &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.borgbackup.org&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;BorgBackup&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I won&amp;rsquo;t spend time to explain how to install it, because the official documentation is very well made.&lt;/p&gt;
&lt;p&gt;I also won&amp;rsquo;t spend time to explain how to configure it, because that&amp;rsquo;s not how I used it.&lt;/p&gt;
&lt;p&gt;Borg is a command line software and using and configuring it is not straightforward, that&amp;rsquo;s why I immediately looked for a UI.&lt;/p&gt;
&lt;h2 id=&#34;vorta&#34;&gt;Vorta&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://www.andreagrandi.it/posts/my-backup-strategy/vorta-ui.png&#34;
	width=&#34;1728&#34;
	height=&#34;1432&#34;
	
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;120&#34;
		data-flex-basis=&#34;289px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Here comes &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://vorta.borgbase.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Vorta&lt;/a&gt;&lt;/strong&gt;. As I mentioned before, while Borg has all the features I need, unfortunately it&amp;rsquo;s not easy to use and configure. A command line utility is a must if you are backing up a remote server, but it&amp;rsquo;s not really user friendly for desktop usage.&lt;/p&gt;
&lt;p&gt;Vorta allows you to &lt;strong&gt;easily configure a new backup&lt;/strong&gt;. All you have to do is &lt;strong&gt;create&lt;/strong&gt; a new &lt;strong&gt;repository&lt;/strong&gt;, set a &lt;strong&gt;password&lt;/strong&gt;, &lt;strong&gt;select&lt;/strong&gt; the files and folders you want to backup and decide from manual backup and regularly &lt;strong&gt;scheduled&lt;/strong&gt; one.&lt;/p&gt;
&lt;p&gt;Optionally (but I strongly suggest you do it) you can flag a couple of options which will verify the &lt;strong&gt;backup integrity&lt;/strong&gt; regularly.&lt;/p&gt;
&lt;p&gt;In case you need to recover your files, you just have to select an existing archive and a folder on your laptop which will be used to mount the volume. At this point you will be able to &lt;strong&gt;access your files&lt;/strong&gt; from the operating system &lt;strong&gt;file manager&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;borgbase&#34;&gt;BorgBase&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://www.andreagrandi.it/posts/my-backup-strategy/borgbase-plans.jpeg&#34;
	width=&#34;2360&#34;
	height=&#34;1496&#34;
	
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;157&#34;
		data-flex-basis=&#34;378px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.borgbase.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;BorgBase&lt;/a&gt;&lt;/strong&gt; is a service which provides &lt;strong&gt;hosted repositories&lt;/strong&gt; for Borg backup.&lt;/p&gt;
&lt;p&gt;They have three commercial plans, starting &lt;strong&gt;from $2/month for 250 GB&lt;/strong&gt; (which is the plan I have) but they also offer a &lt;strong&gt;free plan with 10 GB&lt;/strong&gt; included, so you can test the service as much as you want.&lt;/p&gt;
&lt;p&gt;The same people also develop Vorta, so if you pay for the service you are also making sure the project remains funded.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As you can see, configuring a good backup solution is not that difficult nor expensive. Remember the best day to start backing up is yesterday and today is still better than tomorrow ðŸ˜‰&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
